{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import torch\n",
    "import torchvision\n",
    "import torchaudio\n",
    "from torchvision.datasets.utils import download_url\n",
    "from torchaudio.io import StreamWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FFmpeg library versions\")\n",
    "for k, v in torchaudio.utils.ffmpeg_utils.get_versions().items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchvision.set_video_backend(\"video_reader\")\n",
    "stream = \"video\"\n",
    "# Download video\n",
    "# download_url(\n",
    "#     \"https://github.com/pytorch/vision/blob/main/test/assets/videos/WUzgd7C1pWA.mp4?raw=true\",\n",
    "#     \".\",\n",
    "#     \"WUzgd7C1pWA.mp4\",\n",
    "# )\n",
    "video_path = \"./WUzgd7C1pWA.mp4\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_video(video_object, start=0, end=None, read_video=True, read_audio=True):\n",
    "    if end is None:\n",
    "        end = float(\"inf\")\n",
    "    if end < start:\n",
    "        raise ValueError(\n",
    "            \"end time should be larger than start time, got \"\n",
    "            f\"start time={start} and end time={end}\"\n",
    "        )\n",
    "\n",
    "    video_frames = torch.empty(0)\n",
    "    video_pts = []\n",
    "    if read_video:\n",
    "        video_object.set_current_stream(\"video\")\n",
    "        frames = []\n",
    "        for frame in itertools.takewhile(\n",
    "            lambda x: x[\"pts\"] <= end, video_object.seek(start)\n",
    "        ):\n",
    "            frames.append(frame[\"data\"])\n",
    "            video_pts.append(frame[\"pts\"])\n",
    "        if len(frames) > 0:\n",
    "            video_frames = torch.stack(frames, 0)\n",
    "\n",
    "    audio_frames = torch.empty(0)\n",
    "    audio_pts = []\n",
    "    if read_audio:\n",
    "        video_object.set_current_stream(\"audio\")\n",
    "        frames = []\n",
    "        for frame in itertools.takewhile(\n",
    "            lambda x: x[\"pts\"] <= end, video_object.seek(start)\n",
    "        ):\n",
    "            frames.append(frame[\"data\"])\n",
    "            audio_pts.append(frame[\"pts\"])\n",
    "        if len(frames) > 0:\n",
    "            audio_frames = torch.cat(frames, 0)\n",
    "\n",
    "    return (\n",
    "        video_frames,\n",
    "        audio_frames,\n",
    "        (video_pts, audio_pts),\n",
    "        video_object.get_metadata(),\n",
    "    )\n",
    "\n",
    "# Total number of frames should be 327 for video and 523264 datapoints for audio\n",
    "video = torchvision.io.VideoReader(video_path)\n",
    "vf, af, info, meta = read_video(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE=meta['audio']['framerate']\n",
    "path = \"./test.wav\"\n",
    "s = StreamWriter(path)\n",
    "s.add_audio_stream(\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    num_channels=1,\n",
    ")\n",
    "# 3. Write the data\n",
    "with s.open():\n",
    "    s.write_audio_chunk(0, af)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
